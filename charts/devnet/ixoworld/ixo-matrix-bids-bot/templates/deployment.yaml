apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "ixo-matrix-bids-bot.fullname" . }}
  labels:
    {{- include "ixo-matrix-bids-bot.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "ixo-matrix-bids-bot.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "ixo-matrix-bids-bot.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "ixo-matrix-bids-bot.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      initContainers:
        - name: gcp-backups
          image: google/cloud-sdk:latest
          imagePullPolicy: IfNotPresent
          restartPolicy: Always
          env:
            - name: GOOGLE_APPLICATION_CREDENTIALS
              value: /bot/gcp-key/key.json
          command:
            - "/bin/sh"
            - "-c"
            - |
              # Function to handle errors gracefully
              handle_error() {
                echo "Error occurred: $1"
                echo "Backup container will continue running but skip this backup cycle"
                return 0
              }
              
              # Install dependencies with error handling
              apt-get update && apt-get install -y zip || handle_error "Failed to install dependencies"
              
              # Authenticate with GCP with error handling
              gcloud auth login --cred-file=$GOOGLE_APPLICATION_CREDENTIALS || handle_error "Failed to authenticate with GCP"
              
              echo "Backup container started successfully"
              
              while true; do
                BACKUP_DIR=$(date +'%Y-%m-%d_%H-%M-%S')
                cd {{ .Values.backup.path }}
                
                {{- if .Values.backup.chunking.enabled }}
                # Use configured chunk size in MB
                CHUNK_SIZE_MB={{ .Values.backup.chunking.chunkSizeMB }}
                {{- else }}
                # Default: 1GB chunks for streaming zip efficiency
                CHUNK_SIZE_MB=1024
                {{- end }}
                
                echo "========================================="
                echo "Starting chunked streaming backup: $BACKUP_DIR"
                echo "Target chunk size: ${CHUNK_SIZE_MB}MB"
                echo "========================================="
                
                # Get all files with their sizes
                CHUNK_LIST="/tmp/chunk_files.txt"
                find . -type f -printf "%s %p\n" > "$CHUNK_LIST"
                
                if [ ! -s "$CHUNK_LIST" ]; then
                  echo "No files to backup, skipping..."
                  rm -f "$CHUNK_LIST"
                  sleep 86400
                  continue
                fi
                
                # Count total files and calculate total size
                TOTAL_FILES=$(wc -l < "$CHUNK_LIST")
                TOTAL_SIZE_MB=$(awk '{sum+=$1} END {printf "%.0f", sum/1024/1024}' "$CHUNK_LIST")
                
                echo "Total files: $TOTAL_FILES"
                echo "Total size: ${TOTAL_SIZE_MB}MB"
                
                # Calculate estimated number of chunks
                EST_CHUNKS=$(( (TOTAL_SIZE_MB + CHUNK_SIZE_MB - 1) / CHUNK_SIZE_MB ))
                echo "Estimated chunks: $EST_CHUNKS"
                echo "========================================="
                
                # Process files in size-based chunks with streaming
                CHUNK_NUM=1
                CURRENT_CHUNK_SIZE=0
                CURRENT_CHUNK_FILES="/tmp/current_chunk.txt"
                SUCCESS_COUNT=0
                FAILED_COUNT=0
                
                > "$CURRENT_CHUNK_FILES"
                
                while IFS= read -r line; do
                  FILE_SIZE=$(echo "$line" | awk '{print $1}')
                  FILE_PATH=$(echo "$line" | cut -d' ' -f2-)
                  FILE_SIZE_MB=$(awk "BEGIN {printf \"%.2f\", $FILE_SIZE/1024/1024}")
                  
                  # Add file to current chunk
                  echo "$FILE_PATH" >> "$CURRENT_CHUNK_FILES"
                  CURRENT_CHUNK_SIZE=$(awk "BEGIN {printf \"%.2f\", $CURRENT_CHUNK_SIZE + $FILE_SIZE_MB}")
                  
                  # Check if chunk size limit reached
                  CHUNK_FULL=$(awk "BEGIN {print ($CURRENT_CHUNK_SIZE >= $CHUNK_SIZE_MB)}")
                  
                  if [ "$CHUNK_FULL" -eq 1 ]; then
                    # Stream this chunk to GCS
                    FILE_COUNT=$(wc -l < "$CURRENT_CHUNK_FILES")
                    echo "Processing chunk $CHUNK_NUM: ${FILE_COUNT} files, ${CURRENT_CHUNK_SIZE}MB..."
                    
                    # Use streaming zip: memory-efficient, processes files one-by-one
                    if cat "$CURRENT_CHUNK_FILES" | tr '\n' '\0' | xargs -0 zip -q -r - | \
                       gsutil -o GSUtil:parallel_composite_upload_threshold=150M \
                       cp - {{ .Values.backup.gcs.bucket }}/$BACKUP_DIR/data-${CHUNK_NUM}.zip; then
                      echo "✓ Chunk $CHUNK_NUM uploaded successfully"
                      SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
                    else
                      echo "✗ Chunk $CHUNK_NUM failed (exit code: $?)"
                      FAILED_COUNT=$((FAILED_COUNT + 1))
                    fi
                    
                    # Reset for next chunk
                    CHUNK_NUM=$((CHUNK_NUM + 1))
                    CURRENT_CHUNK_SIZE=0
                    > "$CURRENT_CHUNK_FILES"
                  fi
                done < "$CHUNK_LIST"
                
                # Upload remaining files if any
                if [ -s "$CURRENT_CHUNK_FILES" ]; then
                  FILE_COUNT=$(wc -l < "$CURRENT_CHUNK_FILES")
                  echo "Processing final chunk $CHUNK_NUM: ${FILE_COUNT} files, ${CURRENT_CHUNK_SIZE}MB..."
                  
                  if cat "$CURRENT_CHUNK_FILES" | tr '\n' '\0' | xargs -0 zip -q -r - | \
                     gsutil -o GSUtil:parallel_composite_upload_threshold=150M \
                     cp - {{ .Values.backup.gcs.bucket }}/$BACKUP_DIR/data-${CHUNK_NUM}.zip; then
                    echo "✓ Chunk $CHUNK_NUM uploaded successfully"
                    SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
                  else
                    echo "✗ Chunk $CHUNK_NUM failed (exit code: $?)"
                    FAILED_COUNT=$((FAILED_COUNT + 1))
                  fi
                fi
                
                # Cleanup
                rm -f "$CHUNK_LIST" "$CURRENT_CHUNK_FILES"
                
                # Summary
                TOTAL_CHUNKS=$((SUCCESS_COUNT + FAILED_COUNT))
                AVG_CHUNK_SIZE=$(awk "BEGIN {printf \"%.0f\", $TOTAL_SIZE_MB / $TOTAL_CHUNKS}")
                echo "========================================="
                echo "Backup Summary for $BACKUP_DIR:"
                echo "Total files: $TOTAL_FILES"
                echo "Total size: ${TOTAL_SIZE_MB}MB"
                echo "Total chunks: $TOTAL_CHUNKS"
                echo "Average chunk size: ${AVG_CHUNK_SIZE}MB"
                echo "Successful uploads: $SUCCESS_COUNT"
                echo "Failed uploads: $FAILED_COUNT"
                echo "========================================="
                
                if [ $FAILED_COUNT -eq 0 ]; then
                  echo "✓ All chunks backed up successfully!"
                else
                  echo "⚠ Warning: $FAILED_COUNT chunk(s) failed - will retry in next cycle"
                fi
                
                echo "Sleeping for 24 hours until next backup..."
                sleep 86400 # 24 hours
              done
          volumeMounts:
            - name: gcp-service-account-key
              mountPath: /bot/gcp-key
            - name: storage
              mountPath: {{ .Values.backup.path }}
          resources:
            {{- toYaml .Values.backup.resources | nindent 12 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 9000
              protocol: TCP
            - name: rest
              containerPort: 8083
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: rest
          readinessProbe:
            httpGet:
              path: /
              port: rest
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - name: storage
              mountPath: {{ .Values.backup.path }}
            - name: config
              mountPath: /bot/config
              readOnly: true
      volumes:
        - name: gcp-service-account-key
          secret:
            secretName: gcp-key-secret
        - name: storage
          persistentVolumeClaim:
            claimName: {{ .Release.Name }}-storage
        - name: config
          configMap:
            name: {{ include "ixo-matrix-bids-bot.fullname" . }}-config
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
