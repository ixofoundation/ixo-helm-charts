apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "ixo-matrix-appservice-rooms.fullname" . }}
  labels:
    {{- include "ixo-matrix-appservice-rooms.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "ixo-matrix-appservice-rooms.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "ixo-matrix-appservice-rooms.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "ixo-matrix-appservice-rooms.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      initContainers:
        - name: gcp-backups
          image: google/cloud-sdk:latest
          imagePullPolicy: IfNotPresent
          restartPolicy: Always
          env:
            - name: GOOGLE_APPLICATION_CREDENTIALS
              value: /bot/gcp-key/key.json
          command:
            - "/bin/sh"
            - "-c"
            - |
              # Function to handle errors gracefully
              handle_error() {
                echo "Error occurred: $1"
                echo "Backup container will continue running but skip this backup cycle"
                return 0
              }
              
              # Install dependencies with error handling
              apt-get update && apt-get install -y zip || handle_error "Failed to install dependencies"
              
              # Authenticate with GCP with error handling
              gcloud auth login --cred-file=$GOOGLE_APPLICATION_CREDENTIALS || handle_error "Failed to authenticate with GCP"
              
              echo "Backup container started successfully"
              
              while true; do
                BACKUP_DIR=$(date +'%Y-%m-%d_%H-%M-%S')
                cd {{ .Values.backup.path }}
                
                {{- if .Values.backup.chunking.enabled }}
                # Use configured chunk size in MB
                CHUNK_SIZE_MB={{ .Values.backup.chunking.chunkSizeMB }}
                {{- else }}
                # Default: 1GB chunks for streaming zip efficiency
                CHUNK_SIZE_MB=1024
                {{- end }}
                
                echo "========================================="
                echo "Starting chunked streaming backup: $BACKUP_DIR"
                echo "Target chunk size: ${CHUNK_SIZE_MB}MB"
                echo "========================================="
                
                # Get all files with their sizes
                CHUNK_LIST="/tmp/chunk_files.txt"
                find . -type f -printf "%s %p\n" > "$CHUNK_LIST"
                
                if [ ! -s "$CHUNK_LIST" ]; then
                  echo "No files to backup, skipping..."
                  rm -f "$CHUNK_LIST"
                  sleep 86400
                  continue
                fi
                
                # Count total files and calculate total size
                TOTAL_FILES=$(wc -l < "$CHUNK_LIST")
                TOTAL_SIZE_MB=$(awk '{sum+=$1} END {printf "%.0f", sum/1024/1024}' "$CHUNK_LIST")
                
                echo "Total files: $TOTAL_FILES"
                echo "Total size: ${TOTAL_SIZE_MB}MB"
                
                # Calculate estimated number of chunks
                EST_CHUNKS=$(( (TOTAL_SIZE_MB + CHUNK_SIZE_MB - 1) / CHUNK_SIZE_MB ))
                echo "Estimated chunks: $EST_CHUNKS"
                echo "========================================="
                
                # Accurate size-based chunking using awk (fast single-pass)
                CHUNK_NUM=1
                SUCCESS_COUNT=0
                FAILED_COUNT=0
                CHUNK_SIZE_BYTES=$((CHUNK_SIZE_MB * 1024 * 1024))
                
                echo "Creating size-based chunks of ${CHUNK_SIZE_MB}MB..."
                
                # Use awk to create chunks based on actual cumulative size (FAST!)
                awk -v chunk_size="$CHUNK_SIZE_BYTES" -v prefix="/tmp/backup_chunk_" '
                BEGIN {
                  chunk_num = 0
                  current_size = 0
                  chunk_file = prefix chunk_num ".chunk"
                }
                {
                  file_size = $1
                  file_path = substr($0, index($0, $2))
                  
                  # Start new chunk if adding this file exceeds limit
                  if (current_size > 0 && current_size + file_size > chunk_size) {
                    close(chunk_file)
                    chunk_num++
                    chunk_file = prefix chunk_num ".chunk"
                    current_size = 0
                  }
                  
                  # Add file to current chunk
                  print file_size " " file_path > chunk_file
                  current_size += file_size
                }
                END {
                  if (current_size > 0) close(chunk_file)
                }
                ' "$CHUNK_LIST"
                
                # Process each chunk
                for CHUNK_FILE in /tmp/backup_chunk_*.chunk; do
                  [ -f "$CHUNK_FILE" ] || continue
                  
                  CHUNK_SIZE=$(awk '{sum+=$1} END {printf "%.0f", sum/1024/1024}' "$CHUNK_FILE")
                  FILE_COUNT=$(wc -l < "$CHUNK_FILE")
                  
                  echo "Processing chunk $CHUNK_NUM: $FILE_COUNT files, ${CHUNK_SIZE}MB..."
                  
                  # Extract just file paths and stream to zip
                  if awk '{$1=""; print substr($0,2)}' "$CHUNK_FILE" | \
                     tr '\n' '\0' | \
                     xargs -0 zip -q -r - | \
                     gsutil -o GSUtil:parallel_composite_upload_threshold=150M \
                     cp - {{ .Values.backup.gcs.bucket }}/$BACKUP_DIR/data-${CHUNK_NUM}.zip; then
                    echo "✓ Chunk $CHUNK_NUM uploaded successfully"
                    SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
                  else
                    echo "✗ Chunk $CHUNK_NUM failed (exit code: $?)"
                    FAILED_COUNT=$((FAILED_COUNT + 1))
                  fi
                  
                  rm -f "$CHUNK_FILE"
                  CHUNK_NUM=$((CHUNK_NUM + 1))
                done
                
                # Cleanup
                rm -f "$CHUNK_LIST" /tmp/backup_chunk_*.chunk
                
                # Summary
                TOTAL_CHUNKS=$((SUCCESS_COUNT + FAILED_COUNT))
                AVG_CHUNK_SIZE=$(awk "BEGIN {printf \"%.0f\", $TOTAL_SIZE_MB / $TOTAL_CHUNKS}")
                echo "========================================="
                echo "Backup Summary for $BACKUP_DIR:"
                echo "Total files: $TOTAL_FILES"
                echo "Total size: ${TOTAL_SIZE_MB}MB"
                echo "Total chunks: $TOTAL_CHUNKS"
                echo "Average chunk size: ${AVG_CHUNK_SIZE}MB"
                echo "Successful uploads: $SUCCESS_COUNT"
                echo "Failed uploads: $FAILED_COUNT"
                echo "========================================="
                
                if [ $FAILED_COUNT -eq 0 ]; then
                  echo "✓ All chunks backed up successfully!"
                else
                  echo "⚠ Warning: $FAILED_COUNT chunk(s) failed - will retry in next cycle"
                fi
                
                echo "Sleeping for 24 hours until next backup..."
                sleep 86400 # 24 hours
              done
          volumeMounts:
            - name: gcp-service-account-key
              mountPath: /bot/gcp-key
            - name: storage
              mountPath: {{ .Values.backup.path }}
          resources:
            {{- toYaml .Values.backup.resources | nindent 12 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 9000
              protocol: TCP
            - name: rest
              containerPort: 8081
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: rest
          readinessProbe:
            httpGet:
              path: /
              port: rest
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - name: storage
              mountPath: {{ .Values.backup.path }}
            - name: config
              mountPath: /bot/config
              readOnly: true
            - name: registration
              mountPath: /bot/registration
              readOnly: true
      volumes:
        - name: gcp-service-account-key
          secret:
            secretName: gcp-key-secret
        - name: storage
          persistentVolumeClaim:
            claimName: {{ .Release.Name }}-storage
        - name: config
          configMap:
            name: {{ include "ixo-matrix-appservice-rooms.fullname" . }}-config
        - name: registration
          configMap:
            name: {{ include "ixo-matrix-appservice-rooms.fullname" . }}-registration
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
